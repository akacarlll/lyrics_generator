{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfcaab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lyricsgeniusNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for lyricsgenius from https://files.pythonhosted.org/packages/0d/32/be32f6922f70fd1b9900b50b228f6585cd60a96bdf03589df738f627d388/lyricsgenius-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading lyricsgenius-3.0.1-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in c:\\users\\celes\\anaconda3\\lib\\site-packages (from lyricsgenius) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\celes\\anaconda3\\lib\\site-packages (from lyricsgenius) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\celes\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.6.0->lyricsgenius) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\celes\\anaconda3\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\celes\\anaconda3\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\celes\\anaconda3\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\celes\\anaconda3\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (2024.7.4)\n",
      "Downloading lyricsgenius-3.0.1-py3-none-any.whl (59 kB)\n",
      "   ---------------------------------------- 0.0/59.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/59.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/59.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/59.4 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 51.2/59.4 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 59.4/59.4 kB 393.4 kB/s eta 0:00:00\n",
      "Installing collected packages: lyricsgenius\n",
      "Successfully installed lyricsgenius-3.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b41f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by Radiohead...\n",
      "\n",
      "Song 1: \"1000 miles\"\n",
      "Song 2: \"15 Step\"\n",
      "Song 3: \"15 Step (Live From The Basement)\"\n",
      "\n",
      "Reached user-specified song limit (3).\n",
      "Done. Found 3 songs.\n",
      "[Song(id, artist, ...), Song(id, artist, ...), Song(id, artist, ...)]\n"
     ]
    }
   ],
   "source": [
    "from lyricsgenius import Genius\n",
    "\n",
    "token=\"vEuhOt5S8b7JMcQuXZhlSNouoVIPQruY6a0bsie-lifgyPxsx_dPVzDSZools0pG\"\n",
    "genius = Genius(token)\n",
    "artist = genius.search_artist(\"Radiohead\", max_songs=3, sort=\"title\")\n",
    "print(artist.songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aff3b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"No Surprises\" by Radiohead...\n",
      "Done.\n",
      "146 ContributorsTranslationsРусскийDeutschEspañolFrançaisNo Surprises Lyrics[Verse 1]\n",
      "A heart that's full up like a landfill\n",
      "A job that slowly kills you\n",
      "Bruises that won't heal\n",
      "You look so tired, unhappy\n",
      "Bring down the government\n",
      "They don't, they don't speak for us\n",
      "I'll take a quiet life\n",
      "A handshake of carbon monoxide\n",
      "\n",
      "[Chorus]\n",
      "And no alarms and no surprises\n",
      "No alarms and no surprises\n",
      "No alarms and no surprises\n",
      "Silent, silent\n",
      "[Verse 2]\n",
      "This is my final fit\n",
      "My final bellyache with\n",
      "\n",
      "[Chorus]\n",
      "No alarms and no surprises\n",
      "No alarms and no surprises\n",
      "No alarms and no surprises, please\n",
      "\n",
      "[Verse 3]\n",
      "Such a pretty house\n",
      "And such a pretty garden\n",
      "You might also like[Chorus]\n",
      "No alarms and no surprises\n",
      "(Get me out of here)\n",
      "No alarms and no surprises\n",
      "(Get me out of here)\n",
      "No alarms and no surprises, please\n",
      "(Get me out of here)117Embed\n"
     ]
    }
   ],
   "source": [
    "song = artist.song(\"No Surprises\")\n",
    "\n",
    "print(song.lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a18d8a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Song(id, artist, ...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist.add_song(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "015b31dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Lyrics_Radiohead.json.\n"
     ]
    }
   ],
   "source": [
    "artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efa41c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"The Life of Pablo\" by Kanye West...\n",
      "Wrote Lyrics_TheLifeofPablo.json.\n"
     ]
    }
   ],
   "source": [
    "genius = Genius(token, timeout=10)\n",
    "\n",
    "album = genius.search_album(\"The Life of Pablo\", \"Kanye West\")\n",
    "album.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f569e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for songs by Kanye West...\n",
      "\n",
      "Song 1: \"Mercy\"\n",
      "Song 2: \"Monster\"\n",
      "Song 3: \"Father Stretch My Hands Pt. 1\"\n",
      "Song 4: \"Ultralight Beam\"\n",
      "Song 5: \"Bound 2\"\n",
      "Song 6: \"Runaway\"\n",
      "Song 7: \"New Slaves\"\n",
      "Song 8: \"Blood On the Leaves\"\n",
      "Song 9: \"Black Skinhead\"\n",
      "Song 10: \"No More Parties in LA\"\n",
      "Song 11: \"Famous\"\n",
      "Song 12: \"Ghost Town\"\n",
      "Song 13: \"POWER\"\n",
      "Song 14: \"All Mine\"\n",
      "Song 15: \"Gold Digger\"\n",
      "Song 16: \"Wolves\"\n",
      "Song 17: \"I Love Kanye\"\n",
      "Song 18: \"Real Friends\"\n",
      "Song 19: \"Off The Grid\"\n",
      "Song 20: \"Can’t Tell Me Nothing\"\n",
      "Song 21: \"Devil In a New Dress\"\n",
      "Song 22: \"Saint Pablo\"\n",
      "Song 23: \"Gorgeous\"\n",
      "Song 24: \"All of the Lights\"\n",
      "Song 25: \"Dark Fantasy\"\n",
      "Song 26: \"Pt. 2\"\n",
      "Song 27: \"Hold My Liquor\"\n",
      "Song 28: \"Waves\"\n",
      "Song 29: \"FML\"\n",
      "Song 30: \"Lift Yourself\"\n",
      "Song 31: \"All Day\"\n",
      "Song 32: \"So Appalled\"\n",
      "Song 33: \"Blame Game\"\n",
      "Song 34: \"I Am a God\"\n",
      "Song 35: \"I’m In It\"\n",
      "Song 36: \"Praise God\"\n",
      "Song 37: \"Stronger\"\n",
      "Song 38: \"All Falls Down\"\n",
      "Song 39: \"Facts (Charlie Heat Version)\"\n",
      "Song 40: \"Violent Crimes\"\n",
      "Song 41: \"Through the Wire\"\n",
      "Song 42: \"Flashing Lights\"\n",
      "Song 43: \"I Thought About Killing You\"\n",
      "Song 44: \"Heartless\"\n",
      "Song 45: \"30 Hours\"\n",
      "Song 46: \"Only One\"\n",
      "Song 47: \"Jesus Walks\"\n",
      "Song 48: \"Yikes\"\n",
      "Song 49: \"Homecoming\"\n",
      "Song 50: \"Highlights\"\n",
      "Song 51: \"Jail\"\n",
      "Song 52: \"Fade\"\n",
      "Song 53: \"I Wonder\"\n",
      "Song 54: \"Good Morning\"\n",
      "Song 55: \"On Sight\"\n",
      "Song 56: \"Last Call\"\n",
      "Song 57: \"Wouldn’t Leave\"\n",
      "Song 58: \"Moon\"\n",
      "Song 59: \"Follow God\"\n",
      "Song 60: \"Closed on Sunday\"\n",
      "Song 61: \"Touch the Sky\"\n",
      "Song 62: \"Guilt Trip\"\n",
      "Song 63: \"Lost In the World\"\n",
      "Song 64: \"Good Life\"\n",
      "Song 65: \"Jesus Lord\"\n",
      "Song 66: \"Send It Up\"\n",
      "Song 67: \"Selah\"\n",
      "Song 68: \"Jonah\"\n",
      "Song 69: \"Diamonds From Sierra Leone (Remix)\"\n",
      "Song 70: \"Never Let Me Down\"\n",
      "Song 71: \"No Mistakes\"\n",
      "Song 72: \"Big Brother\"\n",
      "Song 73: \"Never See Me Again\"\n",
      "Song 74: \"Hell of a Life\"\n",
      "Song 75: \"God Is\"\n",
      "Song 76: \"Freestyle 4\"\n",
      "Song 77: \"Feedback\"\n",
      "Song 78: \"Diamonds from Sierra Leone\"\n",
      "Song 79: \"Hey Mama\"\n",
      "Song 80: \"Ok Ok\"\n",
      "Song 81: \"Junya\"\n",
      "Song 82: \"Champion\"\n",
      "Song 83: \"Heard ’Em Say\"\n",
      "Song 84: \"Pure Souls\"\n",
      "Song 85: \"Everything I Am\"\n",
      "Song 86: \"No Child Left Behind\"\n",
      "Song 87: \"Donda\"\n",
      "Song 88: \"Keep My Spirit Alive\"\n",
      "Song 89: \"Spaceship\"\n",
      "Song 90: \"On God\"\n",
      "Song 91: \"Use This Gospel\"\n",
      "Song 92: \"Believe What I Say\"\n",
      "Song 93: \"The New Workout Plan\"\n",
      "Song 94: \"We Don’t Care\"\n",
      "Song 95: \"Come to Life\"\n",
      "Song 96: \"Family Business\"\n",
      "Song 97: \"Ye vs. the People (starring TI as the People)\"\n",
      "Song 98: \"Remote Control\"\n",
      "Song 99: \"24\"\n",
      "Song 100: \"The Morning\"\n",
      "\n",
      "Reached user-specified song limit (100).\n",
      "Done. Found 100 songs.\n",
      "Wrote Kanye_West_lyrics.json.\n"
     ]
    }
   ],
   "source": [
    "from lyricsgenius import Genius\n",
    "\n",
    "token = \"vEuhOt5S8b7JMcQuXZhlSNouoVIPQruY6a0bsie-lifgyPxsx_dPVzDSZools0pG\"\n",
    "genius = Genius(token, timeout=10)\n",
    "\n",
    "# Rechercher et scraper jusqu'à 100 chansons de l'artiste\n",
    "artist = genius.search_artist(\"Kanye West\", max_songs=100)\n",
    "\n",
    "# Sauvegarder les paroles dans un fichier texte\n",
    "artist.save_lyrics(\"Kanye_West_lyrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95a2eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_lyrics(lyrics):\n",
    "    # Supprimer les titres de sections comme \"[Chorus]\" ou \"[Verse 1]\"\n",
    "    lyrics = re.sub(r'\\[.*?\\]', '', lyrics)\n",
    "    # Supprimer les espaces et retours à la ligne en excès\n",
    "    lyrics = re.sub(r'\\n+', '\\n', lyrics).strip()\n",
    "    return lyrics\n",
    "\n",
    "# Appliquer le nettoyage sur les paroles de chaque chanson\n",
    "with open(\"Kanye_West_lyrics.json\", \"r\") as file:\n",
    "    raw_lyrics = file.read()\n",
    "\n",
    "cleaned_lyrics = clean_lyrics(raw_lyrics)\n",
    "\n",
    "# Sauvegarder les paroles nettoyées\n",
    "with open(\"Clean_Kanye_West_lyrics.txt\", \"w\") as file:\n",
    "    file.write(cleaned_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a314c6b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celes\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\celes\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\celes\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\celes\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                        \n\u001b[0;32m     26\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \n\u001b[0;32m     27\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39minputs,  \u001b[38;5;66;03m# Attention: inputs n'est pas directement un Dataset                \u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle (cette étape doit être adaptée)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1812\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1814\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1815\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1816\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:247\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key][item] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key. Only three types of key are available: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Charger le modèle GPT-2 et le tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Lire les paroles nettoyées depuis le fichier texte\n",
    "with open(\"Clean_Kanye_West_lyrics.txt\", \"r\") as file:\n",
    "    cleaned_lyrics = file.read()\n",
    "\n",
    "# Préparer les données d'entraînement\n",
    "inputs = tokenizer(cleaned_lyrics, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Spécifier les paramètres d'entraînement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',        \n",
    "    num_train_epochs=3,             \n",
    "    per_device_train_batch_size=4,  \n",
    "    save_steps=10_000,              \n",
    "    save_total_limit=2,             \n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=inputs,  # Attention: inputs n'est pas directement un Dataset                \n",
    ")\n",
    "\n",
    "# Entraîner le modèle (cette étape doit être adaptée)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6658016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88da589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter un jeton de padding\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizer les données avec padding\n",
    "tokenized_inputs = tokenizer(cleaned_lyrics, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Créer un dataset PyTorch compatible\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"input_ids\": tokenized_inputs[\"input_ids\"], \"attention_mask\": tokenized_inputs[\"attention_mask\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e508385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',        \n",
    "    num_train_epochs=3,             \n",
    "    per_device_train_batch_size=4,  \n",
    "    save_steps=10_000,              \n",
    "    save_total_limit=2,             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d86e547d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=3.1618436177571616, metrics={'train_runtime': 7.9678, 'train_samples_per_second': 0.377, 'train_steps_per_second': 0.377, 'total_flos': 783876096000.0, 'train_loss': 3.1618436177571616, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajuster les étiquettes pour être identiques aux input_ids (pour des tâches de génération)\n",
    "tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "\n",
    "# Créer un dataset avec les labels\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "    \"labels\": tokenized_inputs[\"labels\"]\n",
    "})\n",
    "\n",
    "# Utiliser ce dataset pour l'entraînement\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,  # Utiliser le dataset avec labels\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f615061f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of song is a game, there should be no'realistic' time-saver with the idea that this one word would come and play. What he just used to mean was: it's like having three or more musical pieces each turn while playing all things on his own; you have only half your repertoire before getting stuck in what will become so big for sure everything feels different when played right. If nothing gets broken out then someone can make up as little over 20 minutes if they are lucky but let him do every bit about 80% realisn't! We're not trying too hard by allowing them into our vocabulary first because we think something does go through us who've built some sort of amazing instrument system even though music (and certainly yes) cannot fully accountfully control anything else done without actually throwing oneself at God, also making himself seem somewhat invincible somehow rather than being held responsible from ruining any aspect others learn stuff nice words people love don´t\n",
      "A huge part...I\n"
     ]
    }
   ],
   "source": [
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,  # Longueur des paroles\n",
    "    num_return_sequences=1,  # Nombre de séquences générées\n",
    "    temperature=1.2,  # Diversité plus élevée\n",
    "    top_k=50,  # Limite à 50 mots les plus probables\n",
    "    top_p=0.95,  # Nucleus sampling pour plus de variété\n",
    "    repetition_penalty=1.3,  # Pénaliser les répétitions\n",
    "    do_sample=True  # Active l'échantillonnage pour plus de créativité\n",
    ")\n",
    "\n",
    "# Décoder et afficher les paroles générées\n",
    "generated_lyrics = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "print(generated_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92fb5b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of song: \"I'm a little bit of a girl\"\n",
      "\n",
      "\"I'm a little girl.\"\n",
      "\n",
      "\"I'm a little bit of a girl.\"\n",
      "\n",
      "\"I'm a little girl.\"\n",
      "\n",
      "\"I'm a little girl.\"\n",
      "\n",
      "\"I'm a little girl.\"\n",
      "\"I'm a little girl.\"\n",
      "\n",
      "\"I'm a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n"
     ]
    }
   ],
   "source": [
    "# Générer des paroles\n",
    "input_ids = tokenizer.encode(\"Start of song\", return_tensors=\"pt\")\n",
    "sample_output = model.generate(input_ids, max_length=200, num_return_sequences=1)\n",
    "\n",
    "# Décoder et afficher les paroles générées\n",
    "generated_lyrics = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "print(generated_lyrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
